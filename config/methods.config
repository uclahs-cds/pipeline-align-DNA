includeConfig "../external/nextflow-config/config/csv/csv_parser.config"
includeConfig "../external/nextflow-config/config/methods/common_methods.config"
includeConfig "../external/nextflow-config/config/retry/retry.config"
includeConfig "../external/nextflow-config/config/schema/schema.config"

methods {
    /**
     * Check the permissions and existence of workDir.
     * If it doesn't exist, recursively find first existing directory and check write permission.
     * If it exists, check write permission.
     */
    check_workdir_permissions = { dir ->
        dir_file = new File(dir)
        if (dir_file.exists()) {
            if (dir_file.canWrite()) {
                return true
            } else {
                throw new Exception("   ### ERROR ###   The input directory params.work_dir: ${dir} is not writeable. Please verify and try again.")
            }
        } else {
            while (!dir_file.exists()) {
                dir_file = dir_file.getParentFile()
            }

            if (dir_file.canWrite()) {
                return true
            } else {
                throw new Exception("   ### ERROR ###   The input directory params.work_dir: ${dir} cannot be created. The closest existing parent directory ${dir_file.toString()} is not writable. Please verify permissions or change the input parameter.")
            }
        }
    }

    check_aligner = {

        if (!params.containsKey('aligner')) {
            params.aligner = ["BWA-MEM2"]
            params.reference_fasta_bwa = params.reference_fasta
            params.reference_fasta_index_files_bwa = "${params.reference_fasta_bwa}.*"
            if (!params.containsKey('reference_fasta_index_files_bwa')) {
                params.reference_fasta_index_files_bwa = "${params.reference_fasta_bwa}.fai"
                }
            }
        else {
        if (params.aligner.contains("BWA-MEM2")) {
            if (!params.containsKey('reference_fasta_index_files_bwa')) {
                params.reference_fasta_index_files_bwa = "${params.reference_fasta_bwa}.*"
                }
            if (!params.containsKey('reference_fasta_index_files_bwa')) {
                params.reference_fasta_index_files_bwa = "${params.reference_fasta_bwa}.fai"
                }
            }

        if (params.aligner.contains('HISAT2')) {
            params.reference_fasta_index_files_hisat2 = "${params.hisat2_index_prefix}.*.ht2"
            }
        }
    }

    /**
    *   Extract and properly set the reference genome versions
    */
    extract_genome_versions = {
        def genome_versions = [
                "BWA-MEM2": "no-ver",
                "HISAT2": "no-ver"
            ]
        if (params.containsKey("ucla_cds_reference_genome_version") && params.ucla_cds_reference_genome_version) {
            genome_versions["BWA-MEM2"] = params.ucla_cds_reference_genome_version
            genome_versions["HISAT2"] = params.ucla_cds_reference_genome_version
        } else {
            if (params.aligner.contains("BWA-MEM2")) {
                genome_versions["BWA-MEM2"] = methods.get_genome_version(params.reference_fasta_bwa)
            }
            if (params.aligner.contains("HISAT2")) {
                genome_versions["HISAT2"] = methods.get_genome_version(params.reference_fasta_hisat2)
            }
        }
        return genome_versions
    }

    /* sets the parameters from the input which may be either a YAML (specified with key 'input')
     * or a CSV (specified with key 'input_csv')
     */
    set_params_from_input = {
        if (params.containsKey('input_csv')) {
            def reader = new BufferedReader(new FileReader(params.input_csv))

            header_line = reader.readLine()
            def csv_header_fields = header_line.split(',') as List
            def raw_csv_input = csv_parser.parse_csv(params.input_csv, csv_header_fields)

            // format the raw input so it matches the YAML
            params.input = ['FASTQ': []]

            // extract the inputs from the parsed csv
            def extracted_inputs = [] as Set
            raw_csv_input.each { csv_line ->
                extracted_inputs.add(
                    csv_line
                )
            }

            params.input.FASTQ = extracted_inputs
        } else if (!(params.containsKey('input'))) {
            throw new Exception('Neither YAML nor CSV inputs found! Please run pipeline with inputs.')
        }
    }

    // Set the output directory and filename for output bam and log files. If the input dataset is
    // registered in the blcdes, the output directory is set to the data storage. Otherwise, the
    // ouput directory is set to the params.output_dir
    set_output_dir = {
        def tz = TimeZone.getTimeZone("UTC")
        def date = new Date().format("yyyyMMdd'T'HHmmss'Z'", tz)
        // if the fastq files from inpu.csv are registered blcds datasets, dataset information is
        // read from the fastq path, including disease_id, dataset_id, patient_id, and sample_id.
        if (params.ucla_cds_registered_dataset_input) {
            def fastqs = []
            params.input.FASTQ.each { FASTQ_line ->
                fastqs.add(FASTQ_line['read2_fastq'])
            }

            def pattern = ~/^(?<baseDir>(?<mntDir>\/\w+)\/data\/(?<diseaseId>\w+)\/(?<datasetId>\w+)\/(?<patientId>\w+)\/(?<sampleId>[A-Za-z0-9-]+)\/(?<analyte>.+)\/(?<technology>.+))\/raw\/FASTQ\/.+$/

            // First check if all input fastq files are from the same sample_id
            base_dirs = fastqs.collect {
                def matcher = it =~ pattern
                if (!matcher.matches()) {
                    throw new Exception("The input path ${it} isn't a valid blcds-registered path.")
                    }
                return matcher.group("baseDir")
                }
            .unique(false)

            if (base_dirs.size() > 1) {
                throw new Exception(
                    "Not all input fastq files are from the same blcds-registered sample.\n" +
                    "Please verify."
                    )
                }

            // grep sample informations from input path
            def matcher = fastqs[0] =~ pattern
            matcher.matches()
            def base_dir = matcher.group("baseDir")

            params.blcds_disease_id = matcher.group("diseaseId")
            params.blcds_dataset_id = matcher.group("datasetId")
            params.blcds_patient_id = matcher.group("patientId")
            params.blcds_sample_id  = matcher.group("sampleId")
            params.blcds_mount_dir  = matcher.group("mntDir")
            params.blcds_analyte    = matcher.group("analyte")
            params.blcds_technology = matcher.group("technology")
            if (!(new File(params.blcds_mount_dir).exists())) {
                throw new Exception(
                    "The mount directory \"${params.blcds_mount_dir}\" was not found.\n" +
                    "Please double check the input.csv: ${params.input_csv} "
                    )
                }
        }

        if (params.ucla_cds_registered_dataset_output) {
            def genome_versions = methods.extract_genome_versions()
            genome_versions.each{ aligner, version ->
                params.ucla_cds_reference_genome_version = version
                def aligner_output_dir_base = "output_dir_base_${aligner.toLowerCase()}"
                params["${aligner_output_dir_base}"] = (params.containsKey('ucla_cds_data_dir') && params.ucla_cds_data_dir) ? \
                    methods.generate_registered_output_directory(params.ucla_cds_data_dir) : \
                    methods.generate_registered_output_directory()
                params["log_output_dir_${aligner.toLowerCase()}"] = params["${aligner_output_dir_base}"] + "/log-${manifest.name}-${manifest.version}-${date}/"
                methods.check_workdir_permissions(params["log_output_dir_${aligner.toLowerCase()}"])
            }

            // Set default Nextflow-level log output dir to BWA-MEM2
            // in case the genome versions are different for the aligners
            params.log_output_dir = (params.aligner.contains("BWA-MEM2")) ? params["log_output_dir_bwa-mem2"] : params["log_output_dir_hisat2"]

            params.bwa_mem2_uuid = methods.generate_uuid()
            params.hisat2_uuid = methods.generate_uuid()
        } else {
            params.output_dir_base = "${params.output_dir}/${manifest.name}-${manifest.version}/${params.sample_id}"
            params.log_output_dir = "${params.output_dir_base}/log-${manifest.name}-${manifest.version}-${date}/"
            methods.check_workdir_permissions(params.log_output_dir)
        }
    }

    // Function to ensure that resource requirements don't go beyond
    // a maximum limit
    check_max = { obj, type ->
        if (type == 'memory') {
            try {
                if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                    return params.max_memory as nextflow.util.MemoryUnit
                else
                    return obj
            } catch (all) {
                println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
                return obj
            }
        } else if (type == 'time') {
            try {
                if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                    return params.max_time as nextflow.util.Duration
                else
                    return obj
            } catch (all) {
                println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
                return obj
            }
        } else if (type == 'cpus') {
            try {
                return Math.min(obj, params.max_cpus as int)
            } catch (all) {
                println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
                return obj
            }
        }
    }

    // Function to ensure that resource requirements don't go beyond
    // a maximum limit or below a minimum limit
    // needed for ../external/nextflow-config/config/retry/retry.config
    check_limits = { obj, type ->
        if (type == 'memory') {
            try {
                if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                    return params.max_memory as nextflow.util.MemoryUnit
                else if (obj.compareTo(params.min_memory as nextflow.util.MemoryUnit) == -1)
                    return params.min_memory as nextflow.util.MemoryUnit
                else
                    return obj
            } catch (all) {
                println "   ### WARNING ###   Max memory '${params.max_memory}' or min memory '${params.min_memory}' is not valid! Using default value: $obj"
                return obj
            }
        } else if (type == 'time') {
            try {
                if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                    return params.max_time as nextflow.util.Duration
                else if (obj.compareTo(params.min_time as nextflow.util.Duration) == -1)
                    return params.min_time as nextflow.util.Duration
                else
                    return obj
            } catch (all) {
                println "   ### WARNING ###   Max time '${params.max_time}' or min time '${params.min_time}' is not valid! Using default value: $obj"
                return obj
            }
        } else if (type == 'cpus') {
            try {
                return Math.max( Math.min( obj, params.max_cpus as int ), params.min_cpus as int )
            } catch (all) {
                println "   ### WARNING ###   Max cpus '${params.max_cpus}' or min cpus '${params.min_cpus}' is not valid! Using default value: $obj"
                return obj
            }
        }
    }

    // Resource allocation here. Static node-based allocations included here
    set_resources_allocation = {
        // Function to ensure that resource requirements don't go beyond
        // a maximum limit
        node_cpus = params.max_cpus
        node_memory_GB = params.max_memory.toGiga()
        // Load base.config by default for all pipelines
        includeConfig "${projectDir}/config/base.config"
        if (params.ucla_cds) {
            if (node_cpus == 64) {
                // Check memory for M64 node
                if (node_cpus == 64 && node_memory_GB >= 950 && node_memory_GB <= 1010) {
                    includeConfig "${projectDir}/config/M64.config"
                } else {
                    throw new Exception("   ### ERROR ###   System resources not as expected (cpus=${node_cpus} memory=${node_memory_GB}), unable to assign resources.")
                }
            } else {
                // Check memory for F series node
                if (node_memory_GB >= (node_cpus * 2 * 0.9) && node_memory_GB <= (node_cpus * 2)) {
                    includeConfig "${projectDir}/config/F${node_cpus}.config"
                } else {
                    throw new Exception("   ### ERROR ###   System resources not as expected (cpus=${node_cpus} memory=${node_memory_GB}), unable to assign resources.")
                }
            }
        }
    }

    set_env = {
        if (params.ucla_cds) {
            /**
             * By default, if the /scratch directory exists, set it as the Nextflow working directory
             * and Spark temp directory.
             * If config file specified work_dir, set it as the Nextflow working directory
             * If config file specified spark_temp_dir, set it as the Spark temp directory
             *
             * WARNING: changing these directories can lead to high server latency and
             * potential disk space limitations. Change with caution! Handles creation of
             * directories which don't already exist e.g. '/scratch/test/'
             * The 'workDir' in Nextflow determines the location of intermediate and temporary files.
             */
            params.work_dir = (params.containsKey('work_dir') && params.work_dir) ? params.work_dir : '/scratch'
            if (methods.check_workdir_permissions(params.work_dir)) {
                workDir = params.work_dir
            }

            params.spark_temp_dir = (params.containsKey('spark_temp_dir') && params.spark_temp_dir && methods.check_workdir_permissions(params.spark_temp_dir)) ? params.spark_temp_dir : '/scratch'

        } else {
            // If work_dir was specified as a param and exists or can be created, set workDir. Otherwise, let Nextflow's default behavior dictate workDir
            if (params.containsKey('work_dir') && params.work_dir && methods.check_workdir_permissions(params.work_dir)) {
                workDir = params.work_dir
            } else {
                params.work_dir = "${launchDir}/work"
            }

            // If spark_temp_dir was specified as a param and exists or can be created, set as spark tempdir. Otherwise, set as workDir.
            params.spark_temp_dir = (params.containsKey('spark_temp_dir') && params.spark_temp_dir && methods.check_workdir_permissions(params.spark_temp_dir)) ? params.spark_temp_dir : "${launchDir}/work"
        }
    }

    set_pipeline_logs = {
        trace.enabled = true
        trace.file = "${params.log_output_dir}/nextflow-log/trace.txt"

        timeline.enabled = true
        timeline.file = "${params.log_output_dir}/nextflow-log/timeline.html"

        report.enabled = true
        report.file = "${params.log_output_dir}/nextflow-log/report.html"
    }

    setup = {
        methods.set_params_from_input()
	schema.validate()
        methods.set_output_dir()
        methods.set_env()
        methods.set_pipeline_logs()
        methods.check_aligner()
        methods.set_resources_allocation()
        retry.setup_retry()
        }
    }

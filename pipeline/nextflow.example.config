import java.util.TimeZone

manifest {
    name = "align-DNA"
    author = "Benjamin Carlin"
    description = "alignment pipeline for paired fastqs DNA samples"
    version = "6.1.1"
}

params {
    bwa_version = "bwa-mem2-2.1"
    docker_image_bwa_and_samtools = "blcdsdockerregistry/align-dna:bwa-mem2-2.1_samtools-1.10"
    docker_image_picardtools = "blcdsdockerregistry/align-dna:picardtools-2.23.3"
    docker_image_sha512sum = "blcdsdockerregistry/align-dna:sha512sum-1.0"
    docker_image_validate_params = "blcdsdockerregistry/validate:1.0.0"

    // sample inputs
    sample_name = "name_of_sample_with_no_spaces"
    input_csv = "path/to/input/csv/"
    reference_fasta = "/path/to/fasta/genome.fa"
    reference_fasta_dict = "/path/to/fasta/genome.dict"
    reference_fasta_index_files = "/path/to/fasta/index/files/genome.fa.*"

    // This is only used when `params.blcds_registered_dataset_output` is set to true
    // TODO: should add a validation for genome versions
    reference_genome_version = "genome_version"

    // input/output locations
    output_dir = "where/to/save/outputs/"
    temp_dir = "/local/disk/for/temp/file/dir/"

    // options
    save_intermediate_files = false
    cache_intermediate_pipeline_steps = false

    // resource configuraton for entire pipeline
    max_number_of_parallel_jobs = 1

    // uncomment to manually set ncpus for bwa-mem2
    // bwa_mem_number_of_cpus = 60

    // set to true if the data input fastq files are registered in the Boutros Lab.
    blcds_registered_dataset_input = false
    // set to true to redirect output files directly to the Boutros Lab data storage.
    blcds_registered_dataset_output = false

    // uncooment the following in order to save output bam and log directly to blcds data storage
    // blcds_cluster_slurm = true
    // blcds_disease_id = "disease_id"
    // blcds_dataset_id = "dataset_id"
    // blcds_patient_id = "patient_id"
    // blcds_sample_id  = "sample_id"
    // blcds_analyte = "DNA"
    // blcds_technology = "WGS"
    // blcds_mount_dir = "/data"
}

docker {
    enabled = true
    sudo = false
    runOptions = "-u \$(id -u):\$(id -g)"
}

// Set the output directory and filename for output bam and log files. If the input dataset is
// registered in the blcdes, the output directory is set to the data storage. Otherwise, the
// ouput directory is set to the params.output_dir
def set_output_dir() {
    // if the fastq files from inpu.csv are registered blcds datasets, dataset information is
    // read from the fastq path, including disease_id, dataset_id, patient_id, and sample_id.
    if (params.blcds_registered_dataset_input) {
        def fastqs = []
        def reader = new FileReader(params.input_csv)
        reader.splitEachLine(",") { fields ->
            fastqs.add(fields[8])
        }
        fastqs.removeAt(0)
        def pattern = ~/^(?<baseDir>(?<mntDir>\/\w+)\/data\/(?<diseaseId>\w+)\/(?<datasetId>\w+)\/(?<patientId>\w+)\/(?<sampleId>[A-Za-z0-9-]+)\/(?<analyte>.+)\/(?<technology>.+))\/raw\/FASTQ\/.+$/
        
        // First check if all input fastq files are from the same sample_id
        base_dirs = fastqs.each {
            def matcher = it =~ pattern
            if (!matcher.matches()) {
                throw new Exception("The input path ${it} isn't a valid blcds-registered path.")
            }
            return matcher.group("baseDir")
        }
        .unique(false)
        
        if (base_dirs.size() > 1) {
            throw new Exception(
                "Not all input fastq files are from the same blcds-registered sample.\n" +
                "Please verify."
            )
        }

        // grep sample informations from input path
        def matcher = fastqs[0] =~ pattern
        matcher.matches()
        def base_dir = matcher.group("baseDir")

        params.blcds_disease_id = matcher.group("diseaseId")
        params.blcds_dataset_id = matcher.group("datasetId")
        params.blcds_patient_id = matcher.group("patientId")
        params.blcds_sample_id  = matcher.group("sampleId")
        params.blcds_mount_dir  = matcher.group("mntDir")
        params.blcds_analyte    = matcher.group("analyte")
        params.blcds_technology = matcher.group("technology")
        if (!(new File(params.blcds_mount_dir).exists())) {
            throw new Exception(
                "The mount directory \"${params.blcds_mount_dir}\" was not found.\n" +
                "Please double check the input.csv: ${params.input_csv} "
            )
        }
    } else if (params.blcds_registered_dataset_output) {
        // TODO: need to valid dataset information
        if (!params.containsKey("blcds_disease_id") || !params.containsKey("blcds_dataset_id") ||
            !params.containsKey("blcds_patient_id") || !params.containsKey("blcds_sample_id") ||
            !params.containsKey("blcds_analyte") || !params.containsKey("blcds_technology")) {
            throw new Exception(
                "Please specify the disease_id, patient_id, dataset_id, sample_id, analyte, " +
                "and technology in the config file."
            )
        }
        if (!params.containsKey("blcds_mount_dir")) {
            if (!params.containsKey("blcds_cluster_slurm")) {
                throw new Exception(
                    "Please specify either the `params.blcds_cluster_slurm` or " +
                    "`params.blcds_mount_dir`"
                )
            }
            params.blcds_mount_dir = params.blcds_cluster_slurm ? "/hot" : "/data"   
        }
        if (!(new File(params.blcds_mount_dir).exists())) {
            throw new Exception(
                "The mount directory \"${params.blcds_mount_dir}\" does not exist.\n" +
                "Please verify the config file."
            )
        }
    }

    // set output directly accordingly
    if (params.blcds_registered_dataset_output) {
        def base_dir = "${params.blcds_mount_dir}/data/${params.blcds_disease_id}/${params.blcds_dataset_id}/${params.blcds_patient_id}/${params.blcds_sample_id}/${params.blcds_analyte}/${params.blcds_technology}"
        // using pasific timezone
        tz = TimeZone.getTimeZone("PST")
        def date = new Date().format('yyyyMMdd-HHmmss', tz)
        if (!(new File(base_dir).canWrite())) {
            throw new Exception(
                "No permission to write ${base_dir}\n"
            )
        }
        params.bam_output_dir = "${base_dir}/aligned/${params.reference_genome_version}/${params.bwa_version.toUpperCase()}/BAM"
        params.bam_output_filename = "${params.bwa_version.toUpperCase()}_${params.blcds_dataset_id}_${params.blcds_sample_id}.bam"
        params.log_output_dir = "${params.bam_output_dir}/log/align-DNA-${date}"
    } else {
        params.bam_output_dir = params.output_dir
        params.bam_output_filename = "${params.sample_name}.bam"
        params.log_output_dir = "${params.output_dir}/log"
    }
}

def set_env() {
    // location of Nextflow temp directories  
    workDir = params.temp_dir
    NXF_WORK = params.temp_dir
    NXF_TEMP = params.temp_dir
    NXF_HOME = params.temp_dir
}

def set_timeline() {
    timeline.enabled = true
    timeline.file = "${params.log_output_dir}/timeline.html"
}

def set_trace() {
    trace.enabled = true
    trace.file = "${params.log_output_dir}/trace.txt"
}

def set_report() {
    report.enabled = true
    report.file = "${params.log_output_dir}/report.html"
}

def set_process() {
    // monitor process jobs with local (not slurm) executor
    process.executor = "local"
    // total amount of resources avaible to the pipeline
    process.maxForks = params.max_number_of_parallel_jobs
    // echo stdout of each step to stdout of pipeline
    process.echo = true
    process.cache = params.cache_intermediate_pipeline_steps
}

def set_docker_sudo() {
    if (params.containsKey("blcds_cluster_slurm") && (!params.blcds_cluster_slurm)) {
        docker.sudo = true
    }
    if (params.containsKey("blcds_mount_dir") && (params.blcds_mount_dir == '/data')) {
        docker.sudo = true
    }
}

def setup() {
    set_output_dir()
    set_env()
    set_process()
    set_timeline()
    set_trace()
    set_report()
    set_docker_sudo()
}

setup()

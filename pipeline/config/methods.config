manifest {
    name = "align-DNA"
    author = "Benjamin Carlin"
    description = "alignment pipeline for paired fastqs DNA samples"
    version = "6.0.0"
}

params {
    bwa_version = "bwa-mem2-2.1"
}

docker {
    enabled = true
}

methods {
    // Set the output directory and filename for output bam and log files. If the input dataset is
    // registered in the blcdes, the output directory is set to the data storage. Otherwise, the
    // ouput directory is set to the params.output_dir
    set_output_dir = {
        // if the fastq files from inpu.csv are registered blcds datasets, dataset information is
        // read from the fastq path, including disease_id, dataset_id, patient_id, and sample_id.
        if (params.blcds_registered_dataset_input) {
            def fastqs = []
            def reader = new FileReader(params.input_csv)
            reader.splitEachLine(",") { fields ->
                fastqs.add(fields[8])
            }
            fastqs.removeAt(0)
            def pattern = ~/^(?<baseDir>(?<mntDir>\/\w+)\/data\/(?<diseaseId>\w+)\/(?<datasetId>\w+)\/(?<patientId>\w+)\/(?<sampleId>[A-Za-z0-9-]+)\/.+)\/raw\/FASTQ\/.+$/
            
            // First check if all input fastq files are from the same sample_id
            base_dirs = fastqs.each {
                def matcher = it =~ pattern
                if (!matcher.matches()) {
                    throw new Exception("The input path ${it} isn't a valid blcds-registered path.")
                }
                return matcher.group("baseDir")
            }
            .unique(false)
            
            if (base_dirs.size() > 1) {
                throw new Exception(
                    "Not all input fastq files are from the same blcds-registered sample.\n" +
                    "Please verify."
                )
            }

            // grep sample informations from input path
            def matcher = fastqs[0] =~ pattern
            matcher.matches()
            def base_dir = matcher.group("baseDir")

            params.blcds_disease_id = matcher.group("diseaseId")
            params.blcds_dataset_id = matcher.group("datasetId")
            params.blcds_patient_id = matcher.group("patientId")
            params.blcds_sample_id  = matcher.group("sampleId")
            params.blcds_mount_dir  = matcher.group("mntDir")
            if (!(new File(params.blcds_mount_dir).exists())) {
                throw new Exception(
                    "The Avere mount directory \"${params.blcds_mount_dir}\" was not found.\n" +
                    "Please double check the input.csv: ${params.input_csv} "
                )
            }
        } else {
            // TODO: need to valid dataset information
            if (!params.containsKey("blcds_disease_id") || !params.containsKey("blcds_dataset_id") ||
                !params.containsKey("blcds_patient_id") || !params.containsKey("blcds_sample_id") ||
                !params.containsKey("blcds_cluster_slurm")) {
                throw new Exception(
                    "Please specify the disease_id, patient_id, dataset_id, and sample_id, and " +
                    "whether the pipeline is running on the slurm cluster in the config file."
                )
            }
            params.blcds_mount_dir = params.blcds_cluster_slurm ? "/hot" : "/data"
            if (!(new File(params.blcds_mount_dir).exists())) {
                throw new Exception(
                    "The Avere mount directory \"${params.blcds_mount_dir}\" does not exist.\n" +
                    "Forgort to set the `params.blcds_mount_dir`?"
                )
            }
        }

        // set output directly accordingly
        if (params.blcds_registered_dataset_output) {
            def base_dir = "${params.blcds_mount_dir}/data/${params.blcds_disease_id}/${params.blcds_dataset_id}/${params.blcds_patient_id}/${params.blcds_sample_id}"
            def date = new Date().format( 'yyyy-MM-dd' )
            if (!(new File(base_dir).canWrite())) {
                throw new Exception(
                    "No permission to write ${base_dir}\n"
                )
            }
            params.bam_output_dir = "${base_dir}/aligned/${params.reference_genome_version}/BAM/"
            params.bam_output_filename = "${params.bwa_version.toUpperCase()}_${params.blcds_dataset_id}-${params.blcds_sample_id}.bam"
            params.log_output_dir = "${base_dir}/aligned/${params.reference_genome_version}/log/align-DNA/${date}/"
        } else {
            params.bam_output_dir = params.output_dir
            params.bam_output_filename = "${params.sample_name}.bam"
            params.log_output_dir = "${params.output_dir}/log"
        }
    }

    set_env = {
        // location of Nextflow temp directories  
        workDir = params.temp_dir
        NXF_WORK = params.temp_dir
        NXF_TEMP = params.temp_dir
        NXF_HOME = params.temp_dir
    }

    set_timeline = {
        timeline.enabled = true
        timeline.file = "${params.log_output_dir}/timeline.html"
    }

    set_trace = {
        trace.enabled = true
        trace.file = "${params.log_output_dir}/trace.txt"
    }

    set_report = {
        report.enabled = true
        report.file = "${params.log_output_dir}/report.html"
    }

    set_process = {
        // monitor process jobs with local (not slurm) executor
        process.executor = "local"
        // total amount of resources avaible to the pipeline
        process.maxForks = params.max_number_of_parallel_jobs
        // echo stdout of each step to stdout of pipeline
        process.echo = true
        process.cache = params.cache_intermediate_pipeline_steps
    }

    setup = {
        methods.set_output_dir()
        methods.set_env()
        methods.set_process()
        methods.set_timeline()
        methods.set_trace()
        methods.set_report()
    }
}

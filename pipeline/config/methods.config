import java.util.TimeZone
import nextflow.util.SysHelper

manifest {
    name = "align-DNA"
    author = "Chenghao Zhu; Aaron Holmes"
    description = "alignment pipeline for paired fastqs DNA samples"
    version = "6.1.0"
}

params {
    // resource configuraton for entire pipeline
    max_number_of_parallel_jobs = 1

    // tools and their versions
    bwa_version = "bwa-mem2-2.1"
}

docker {
    enabled = true
    sudo = false
    runOptions = "-u \$(id -u):\$(id -g)"
}

methods {
    // Set the output directory and filename for output bam and log files. If the input dataset is
    // registered in the blcdes, the output directory is set to the data storage. Otherwise, the
    // ouput directory is set to the params.output_dir
    set_output_dir = {
        // if the fastq files from inpu.csv are registered blcds datasets, dataset information is
        // read from the fastq path, including disease_id, dataset_id, patient_id, and sample_id.
        if (params.blcds_registered_dataset_input) {
            def fastqs = []
            def reader = new FileReader(params.input_csv)
            reader.splitEachLine(",") { fields ->
                fastqs.add(fields[8])
            }
            fastqs.removeAt(0)
            def pattern = ~/^(?<baseDir>(?<mntDir>\/\w+)\/data\/(?<diseaseId>\w+)\/(?<datasetId>\w+)\/(?<patientId>\w+)\/(?<sampleId>[A-Za-z0-9-]+)\/(?<analyte>.+)\/(?<technology>.+))\/raw\/FASTQ\/.+$/
            
            // First check if all input fastq files are from the same sample_id
            base_dirs = fastqs.each {
                def matcher = it =~ pattern
                if (!matcher.matches()) {
                    throw new Exception("The input path ${it} isn't a valid blcds-registered path.")
                }
                return matcher.group("baseDir")
            }
            .unique(false)
            
            if (base_dirs.size() > 1) {
                throw new Exception(
                    "Not all input fastq files are from the same blcds-registered sample.\n" +
                    "Please verify."
                )
            }

            // grep sample informations from input path
            def matcher = fastqs[0] =~ pattern
            matcher.matches()
            def base_dir = matcher.group("baseDir")

            params.blcds_disease_id = matcher.group("diseaseId")
            params.blcds_dataset_id = matcher.group("datasetId")
            params.blcds_patient_id = matcher.group("patientId")
            params.blcds_sample_id  = matcher.group("sampleId")
            params.blcds_mount_dir  = matcher.group("mntDir")
            params.blcds_analyte    = matcher.group("analyte")
            params.blcds_technology = matcher.group("technology")
            if (!(new File(params.blcds_mount_dir).exists())) {
                throw new Exception(
                    "The mount directory \"${params.blcds_mount_dir}\" was not found.\n" +
                    "Please double check the input.csv: ${params.input_csv} "
                )
            }
        } else if (params.blcds_registered_dataset_output) {
            // TODO: need to valid dataset information
            if (!params.containsKey("blcds_disease_id") || !params.containsKey("blcds_dataset_id") ||
                !params.containsKey("blcds_patient_id") || !params.containsKey("blcds_sample_id") ||
                !params.containsKey("blcds_analyte") || !params.containsKey("blcds_technology")) {
                throw new Exception(
                    "Please specify the disease_id, patient_id, dataset_id, sample_id, analyte, " +
                    "and technology in the config file."
                )
            }
            if (!params.containsKey("blcds_mount_dir")) {
                if (!params.containsKey("blcds_cluster_slurm")) {
                    throw new Exception(
                        "Please specify either the `params.blcds_cluster_slurm` or " +
                        "`params.blcds_mount_dir`"
                    )
                }
                params.blcds_mount_dir = params.blcds_cluster_slurm ? "/hot" : "/data"   
            }
            if (!(new File(params.blcds_mount_dir).exists())) {
                throw new Exception(
                    "The mount directory \"${params.blcds_mount_dir}\" does not exist.\n" +
                    "Please verify the config file."
                )
            }
        }

        // set output directly accordingly
        if (params.blcds_registered_dataset_output) {
            def base_dir = "${params.blcds_mount_dir}/data/${params.blcds_disease_id}/${params.blcds_dataset_id}/${params.blcds_patient_id}/${params.blcds_sample_id}/${params.blcds_analyte}/${params.blcds_technology}"
            // using pasific timezone
            tz = TimeZone.getTimeZone("PST")
            def date = new Date().format('yyyyMMdd-HHmmss', tz)
            if (!(new File(base_dir).canWrite())) {
                throw new Exception(
                    "No permission to write ${base_dir}\n"
                )
            }
            params.bam_output_dir = "${base_dir}/aligned/${params.reference_genome_version}/${params.bwa_version.toUpperCase()}/BAM"
            params.bam_output_filename = "${params.bwa_version.toUpperCase()}_${params.blcds_dataset_id}_${params.blcds_sample_id}.bam"
            params.log_output_dir = "${params.bam_output_dir}/log/align-DNA-${date}"
        } else {
            params.bam_output_dir = params.output_dir
            params.bam_output_filename = "${params.sample_name}.bam"
            params.log_output_dir = "${params.output_dir}/log"
        }
    }

    set_env = {
        // location of Nextflow temp directories  
        workDir = params.temp_dir
        NXF_WORK = params.temp_dir
        NXF_TEMP = params.temp_dir
        NXF_HOME = params.temp_dir
    }

    set_timeline = {
        timeline.enabled = true
        timeline.file = "${params.log_output_dir}/timeline.html"
    }

    set_trace = {
        trace.enabled = true
        trace.file = "${params.log_output_dir}/trace.txt"
    }

    set_report = {
        report.enabled = true
        report.file = "${params.log_output_dir}/report.html"
    }

    set_process = {
        // monitor process jobs with local (not slurm) executor
        process.executor = "local"
        // total amount of resources avaible to the pipeline
        process.maxForks = params.max_number_of_parallel_jobs
        // echo stdout of each step to stdout of pipeline
        process.echo = true
        process.cache = params.cache_intermediate_pipeline_steps
    }

    set_docker_sudo = {
        if (params.containsKey("blcds_cluster_slurm") && (!params.blcds_cluster_slurm)) {
            docker.sudo = true
        }
        if (params.containsKey("blcds_mount_dir") && (params.blcds_mount_dir == '/data')) {
            docker.sudo = true
        }
    }

    set_node_config = {
        def node_cpus = SysHelper.getAvailCpus()
        def node_mem  = SysHelper.getAvailMemory().toString()

        //if (node_cpus == 2 && (node_mem == '3 GB' || node_mem == '3.9 GB')) {
            includeConfig 'config/lowmem.config'
        //} else if (node_cpus == 72 && (node_mem == '136.8 GB' || node_mem == '141.7 GB')) {
        //    includeConfig 'config/midmem.config'
        //} else if (node_cpus == 64 && (node_mem == '950 GB' || node_mem == '1007.9 GB')) {
        //    includeConfig 'config/execute.config'
        //} else {
        //    throw new Exception('ERROR: System resources not as expected, unable to assign resources.')
        //}
    }

    setup = {
        methods.set_output_dir()
        methods.set_env()
        methods.set_process()
        methods.set_timeline()
        methods.set_trace()
        methods.set_report()
        methods.set_docker_sudo()
        methods.set_node_config()
    }
}
